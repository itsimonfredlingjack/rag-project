# Database Optimization Analysis - ChromaDB

**Analysis Date:** 2026-01-11  
**Database:** ChromaDB (Vector Database)  
**Collections:** Multiple collections for different document types

## Executive Summary

**Overall Database Status:** ðŸŸ¡ **MODERATE** (Several optimization opportunities)

ChromaDB is a vector database optimized for similarity search, but several query and schema optimizations can improve performance:

- âœ… Parallel queries already implemented (from previous optimization)
- ðŸŸ¡ Missing metadata indexes for WHERE filters
- ðŸŸ¡ No query result caching
- ðŸŸ¡ Connection pooling not optimized
- ðŸŸ¡ Metadata filtering could be more efficient

---

## 1. Query Performance Analysis

### ðŸ”´ CRITICAL: Missing Metadata Indexes

**Issue:** ChromaDB metadata filtering uses sequential scans

**File:** `retrieval_service.py:579`, `retrieval_orchestrator.py:230`

**Current Query Pattern:**
```python
collection.query(
    query_embeddings=[query_embedding],
    n_results=k,
    where=where_filter,  # Metadata filter - no index!
    include=["metadatas", "documents", "distances"],
)
```

**Problem:**
- `where_filter` on metadata fields (doc_type, source, date) causes full collection scans
- No indexes on metadata fields
- ChromaDB doesn't automatically index metadata

**Impact:** 
- Slow queries when filtering by metadata (50-200ms â†’ 500-2000ms)
- Degrades with collection size

**Solution:** Pre-filter or use collection partitioning

**Optimization:**
```python
# Option 1: Use separate collections per doc_type
# Instead of: where={"doc_type": "regeringsbeslut"}
# Use: collection = get_collection("regeringsbeslut")

# Option 2: Pre-filter before query
if where_filter and "doc_type" in where_filter:
    collection_name = f"{where_filter['doc_type']}_collection"
    collection = client.get_collection(collection_name)
    # Remove doc_type from where_filter
    where_filter.pop("doc_type")
```

**Expected Improvement:** 70-90% faster for filtered queries
**Effort:** 2-3 days
**Priority:** HIGH

---

### ðŸŸ¡ MEDIUM: Unnecessary Data Fetching

**File:** `retrieval_service.py:580`, `retrieval_orchestrator.py:231`

**Issue:** Always fetching full documents even when only snippets needed

**Current:**
```python
include=["metadatas", "documents", "distances"]  # Always fetch all
```

**Optimization:** Selective field fetching
```python
# For initial search - only need snippets
include=["metadatas", "distances"]  # Skip full documents

# Only fetch full documents when needed (detail view)
if need_full_document:
    full_doc = collection.get(ids=[doc_id], include=["documents"])
```

**Expected Improvement:** 20-40% faster queries, 30-50% less memory
**Effort:** 1-2 days
**Priority:** MEDIUM

---

### ðŸŸ¡ MEDIUM: No Query Result Caching

**Issue:** Same queries executed repeatedly without caching

**File:** `retrieval_service.py:search()`, `retrieval_orchestrator.py:search()`

**Current:** Every query hits ChromaDB

**Optimization:** Add Redis cache for query results
```python
async def search(self, query, k, ...):
    # Create cache key from query + params
    cache_key = f"query:{hash(query)}:k:{k}:strategy:{strategy}"
    
    # Check cache
    cached = await redis.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Execute query
    result = await self._execute_search(query, k, ...)
    
    # Cache result (TTL: 1 hour for queries, 24 hours for common queries)
    ttl = 3600 if is_common_query(query) else 1800
    await redis.setex(cache_key, ttl, json.dumps(result))
    
    return result
```

**Expected Improvement:** 80-95% faster for repeated queries
**Effort:** 2-3 days
**Priority:** MEDIUM

---

### ðŸŸ¡ MEDIUM: Large Result Set Processing

**File:** `retrieval_service.py:458`, `retrieval_orchestrator.py:370`

**Issue:** Fetching more results than needed, then slicing

**Current:**
```python
# Fetch k*2 results, then slice
query_results = collection.query(..., n_results=k*2)
results = query_results[:k]  # Discard half
```

**Optimization:** Request exact number needed
```python
# Request exactly k results
query_results = collection.query(..., n_results=k)
```

**Expected Improvement:** 10-20% faster, less memory
**Effort:** 1 day
**Priority:** LOW-MEDIUM

---

## 2. Index Strategy

### ChromaDB Index Characteristics

**Vector Indexes:**
- âœ… Automatically created for embeddings
- âœ… HNSW (Hierarchical Navigable Small World) index
- âœ… Optimized for similarity search

**Metadata Indexes:**
- âŒ NOT automatically created
- âŒ Sequential scan for WHERE filters
- âŒ No composite indexes

### ðŸŸ¡ MEDIUM: Collection Partitioning Strategy

**Current:** Single collections with metadata filtering

**Optimized:** Separate collections per document type

**Before:**
```python
# Single collection with metadata
collection.query(
    where={"doc_type": "regeringsbeslut"},
    ...
)
```

**After:**
```python
# Separate collections
regeringsbeslut_collection = client.get_collection("regeringsbeslut")
regeringsbeslut_collection.query(...)  # No WHERE filter needed
```

**Benefits:**
- Faster queries (no metadata filtering)
- Better index utilization
- Easier scaling
- Collection-level optimizations

**Migration Script:**
```python
# Migrate existing data
def migrate_to_partitioned_collections():
    old_collection = client.get_collection("all_documents")
    all_docs = old_collection.get()
    
    # Group by doc_type
    by_type = {}
    for i, doc_type in enumerate(all_docs["metadatas"]):
        dt = doc_type.get("doc_type", "other")
        if dt not in by_type:
            by_type[dt] = {"ids": [], "documents": [], "metadatas": []}
        by_type[dt]["ids"].append(all_docs["ids"][i])
        by_type[dt]["documents"].append(all_docs["documents"][i])
        by_type[dt]["metadatas"].append(all_docs["metadatas"][i])
    
    # Create new collections
    for doc_type, data in by_type.items():
        new_collection = client.get_or_create_collection(
            name=doc_type,
            metadata={"doc_type": doc_type}
        )
        new_collection.add(
            ids=data["ids"],
            documents=data["documents"],
            metadatas=data["metadatas"]
        )
```

**Expected Improvement:** 50-80% faster filtered queries
**Effort:** 3-5 days
**Priority:** MEDIUM

---

### ðŸŸ¡ MEDIUM: Composite Metadata Queries

**Issue:** Multiple metadata filters cause sequential scans

**Current:**
```python
where={
    "doc_type": "regeringsbeslut",
    "source": "Regeringen",
    "date": {"$gte": "2024-01-01"}
}
```

**Optimization:** Use collection partitioning + date range optimization
```python
# Use partitioned collection
collection = get_collection("regeringsbeslut")

# For date ranges, use separate date-indexed collections or pre-filter
# Option: Create monthly collections
collection = get_collection("regeringsbeslut_2024_01")
```

**Expected Improvement:** 60-90% faster for complex filters
**Effort:** 4-6 days
**Priority:** LOW-MEDIUM

---

## 3. Schema Design Optimization

### ðŸŸ¡ MEDIUM: Metadata Schema Normalization

**Current:** Flat metadata structure
```python
metadata = {
    "doc_type": "regeringsbeslut",
    "source": "Regeringen",
    "date": "2024-01-15",
    "title": "...",
    "author": "...",
    # ... many fields
}
```

**Optimization:** Normalize common fields, denormalize for query performance

**Recommended Schema:**
```python
# Collection-level metadata (in collection metadata)
collection_metadata = {
    "doc_type": "regeringsbeslut",
    "source": "Regeringen",
    "date_range": "2024-01-01/2024-12-31"
}

# Document-level metadata (minimal)
document_metadata = {
    "date": "2024-01-15",  # For sorting/filtering
    "title": "...",  # For display
    "id": "doc_123"  # Unique identifier
}
```

**Expected Improvement:** 
- Smaller metadata size (30-50% reduction)
- Faster queries (less data to scan)
- Better cache efficiency

**Effort:** 2-3 days
**Priority:** LOW-MEDIUM

---

### ðŸŸ¡ MEDIUM: Document Chunking Strategy

**Issue:** Large documents stored as single vectors

**Current:** One document = one vector

**Optimization:** Chunk large documents
```python
# Split large documents into chunks
def chunk_document(text, chunk_size=500, overlap=50):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        chunks.append({
            "text": chunk,
            "chunk_index": i // (chunk_size - overlap),
            "total_chunks": len(text) // (chunk_size - overlap)
        })
    return chunks

# Store chunks with parent reference
metadata = {
    "doc_id": "doc_123",
    "chunk_index": 0,
    "parent_doc": "doc_123"
}
```

**Benefits:**
- Better retrieval precision
- Smaller vectors (faster search)
- More granular relevance

**Expected Improvement:** 20-30% better relevance, 10-15% faster search
**Effort:** 5-7 days
**Priority:** LOW

---

## 4. Connection & Pooling Optimization

### ðŸŸ¡ MEDIUM: Single PersistentClient Instance

**File:** `retrieval_service.py:270`

**Current:**
```python
self._chromadb_client = chromadb.PersistentClient(
    path=chroma_path,
    settings=settings,
)
```

**Issue:** 
- Single connection for all operations
- No connection pooling
- Synchronous operations block event loop

**Optimization:** Already partially optimized with `run_in_executor()`

**Additional Optimization:** Connection pool for multiple clients
```python
# Connection pool (if using HTTP client)
from chromadb import HttpClient

# Pool of clients for concurrent operations
self._client_pool = [
    HttpClient(host=host, port=port)
    for _ in range(pool_size)
]

# Round-robin or least-used selection
def get_client():
    return self._client_pool[hash(request_id) % len(self._client_pool)]
```

**Expected Improvement:** Better concurrency handling
**Effort:** 2-3 days
**Priority:** LOW-MEDIUM

---

### ðŸŸ¡ MEDIUM: Async Wrapper for ChromaDB

**Issue:** ChromaDB operations are synchronous

**Current:** Using `run_in_executor()` (good, but can be improved)

**Optimization:** Create async wrapper with better error handling
```python
class AsyncChromaDB:
    def __init__(self, client):
        self.client = client
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def query(self, *args, **kwargs):
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            lambda: self.client.query(*args, **kwargs)
        )
```

**Expected Improvement:** Better resource management
**Effort:** 1-2 days
**Priority:** LOW

---

## 5. Query Pattern Optimization

### âœ… FIXED: N+1 Query Problem

**Status:** Already fixed in previous optimization

**File:** `retrieval_service.py:570-620`

**Before:** Sequential collection queries
**After:** Parallel queries with `asyncio.gather()`

**Improvement:** 50-70% faster for multi-collection searches

---

### ðŸŸ¡ MEDIUM: Batch Operations

**Issue:** Individual document operations

**Current:** One document at a time for updates/deletes

**Optimization:** Batch operations
```python
# Instead of:
for doc_id in doc_ids:
    collection.update(ids=[doc_id], metadatas=[metadata])

# Use:
collection.update(ids=doc_ids, metadatas=metadatas_list)
```

**Expected Improvement:** 60-80% faster for bulk operations
**Effort:** 1 day
**Priority:** LOW

---

### ðŸŸ¡ MEDIUM: Query Result Deduplication

**File:** `retrieval_orchestrator.py:354-361`

**Current:** Deduplication after fetching all results

**Optimization:** Use ChromaDB's built-in deduplication or optimize algorithm
```python
# Current: O(nÂ²) deduplication
seen_ids = {}
for r in all_results:
    if r["id"] not in seen_ids:
        seen_ids[r["id"]] = r

# Optimized: Use set for O(1) lookup
seen_ids = set()
unique_results = []
for r in all_results:
    if r["id"] not in seen_ids:
        seen_ids.add(r["id"])
        unique_results.append(r)
```

**Expected Improvement:** 10-20% faster for large result sets
**Effort:** 1 day
**Priority:** LOW

---

## 6. Caching Strategy

### ðŸ”´ HIGH PRIORITY: Query Result Caching

**Implementation:**
```python
import hashlib
import json
import redis.asyncio as redis

class CachedRetrievalService:
    def __init__(self, retrieval_service, redis_client):
        self.retrieval = retrieval_service
        self.redis = redis_client
    
    async def search(self, query, k, strategy, **kwargs):
        # Create cache key
        cache_params = {
            "query": query,
            "k": k,
            "strategy": str(strategy),
            **kwargs
        }
        cache_key = f"retrieval:{hashlib.sha256(json.dumps(cache_params, sort_keys=True).encode()).hexdigest()}"
        
        # Check cache
        cached = await self.redis.get(cache_key)
        if cached:
            logger.debug(f"Cache hit for query: {query[:50]}...")
            return json.loads(cached)
        
        # Execute query
        result = await self.retrieval.search(query, k, strategy, **kwargs)
        
        # Cache result
        ttl = 3600 if self._is_common_query(query) else 1800
        await self.redis.setex(cache_key, ttl, json.dumps(result, default=str))
        
        return result
```

**Expected Improvement:** 80-95% faster for repeated queries
**Effort:** 2-3 days
**Priority:** HIGH

---

### ðŸŸ¡ MEDIUM: Embedding Cache

**File:** `embedding_service.py`

**Issue:** Same text embedded repeatedly

**Optimization:** Cache embeddings in Redis
```python
async def embed(self, text: str):
    cache_key = f"embed:{hashlib.sha256(text.encode()).hexdigest()}"
    
    cached = await redis.get(cache_key)
    if cached:
        return np.frombuffer(cached, dtype=np.float32)
    
    embedding = self._model.encode(text)
    await redis.setex(cache_key, 86400, embedding.tobytes())
    return embedding
```

**Expected Improvement:** 90%+ faster for repeated text
**Effort:** 2-3 days
**Priority:** MEDIUM

---

## 7. Monitoring & Profiling

### Query Performance Monitoring

**Add logging for slow queries:**
```python
async def search(self, query, k, ...):
    start = time.perf_counter()
    result = await self._execute_search(query, k, ...)
    latency = (time.perf_counter() - start) * 1000
    
    if latency > 500:  # Log slow queries
        logger.warning(f"Slow query: {latency:.1f}ms - query: {query[:50]}")
    
    return result
```

### ChromaDB Statistics

**Monitor collection sizes:**
```python
def get_collection_stats(collection):
    return {
        "count": collection.count(),
        "size_mb": estimate_size(collection),
        "avg_doc_length": calculate_avg_length(collection),
    }
```

---

## Prioritized Optimization Plan

### Phase 1: Quick Wins (1-2 weeks)

1. **Add Query Result Caching** (HIGH)
   - Effort: 2-3 days
   - Impact: 80-95% faster for repeats
   - Requires: Redis setup

2. **Optimize Data Fetching** (MEDIUM)
   - Effort: 1-2 days
   - Impact: 20-40% faster, 30-50% less memory
   - Low risk

3. **Collection Partitioning** (MEDIUM)
   - Effort: 3-5 days
   - Impact: 50-80% faster filtered queries
   - Requires: Migration script

### Phase 2: Schema Optimization (2-3 weeks)

4. **Metadata Schema Normalization** (LOW-MEDIUM)
   - Effort: 2-3 days
   - Impact: 30-50% smaller metadata

5. **Document Chunking** (LOW)
   - Effort: 5-7 days
   - Impact: 20-30% better relevance

### Phase 3: Advanced Optimizations (3-4 weeks)

6. **Connection Pooling** (LOW-MEDIUM)
   - Effort: 2-3 days
   - Better concurrency

7. **Batch Operations** (LOW)
   - Effort: 1 day
   - 60-80% faster bulk ops

---

## Migration Scripts

### Script 1: Partition Collections by doc_type

```python
# migrate_to_partitioned.py
import chromadb
from collections import defaultdict

def migrate_collections():
    client = chromadb.PersistentClient(path="./chromadb_data")
    
    # Get all documents from existing collections
    for old_collection_name in ["all_documents", "main"]:
        try:
            old_col = client.get_collection(old_collection_name)
            all_data = old_col.get()
            
            # Group by doc_type
            by_type = defaultdict(lambda: {"ids": [], "documents": [], "metadatas": []})
            
            for i in range(len(all_data["ids"])):
                doc_type = all_data["metadatas"][i].get("doc_type", "other")
                by_type[doc_type]["ids"].append(all_data["ids"][i])
                by_type[doc_type]["documents"].append(all_data["documents"][i])
                by_type[doc_type]["metadatas"].append(all_data["metadatas"][i])
            
            # Create new partitioned collections
            for doc_type, data in by_type.items():
                new_col = client.get_or_create_collection(
                    name=doc_type,
                    metadata={"doc_type": doc_type, "migrated_from": old_collection_name}
                )
                new_col.add(
                    ids=data["ids"],
                    documents=data["documents"],
                    metadatas=data["metadatas"]
                )
                print(f"Migrated {len(data['ids'])} documents to {doc_type} collection")
        
        except Exception as e:
            print(f"Error migrating {old_collection_name}: {e}")

if __name__ == "__main__":
    migrate_collections()
```

---

## Performance Estimates

### Current Performance

| Operation | Latency | Complexity |
|-----------|---------|------------|
| Single collection query | 50-150ms | O(k log N) |
| Multi-collection query (3 collections) | 200-600ms | O(n Ã— k log N) |
| Filtered query (with WHERE) | 500-2000ms | O(N) sequential scan |
| Repeated query (no cache) | 50-150ms | O(k log N) |

### Optimized Performance

| Operation | Latency | Improvement |
|-----------|---------|-------------|
| Single collection query | 50-150ms | - |
| Multi-collection query (parallel) | 100-200ms | 50-70% faster |
| Filtered query (partitioned) | 100-300ms | 70-90% faster |
| Repeated query (cached) | 5-15ms | 80-95% faster |

---

## Conclusion

**Key Recommendations:**

1. **Immediate:** Add query result caching (HIGH impact, MEDIUM effort)
2. **Short-term:** Optimize data fetching, implement collection partitioning
3. **Long-term:** Schema normalization, document chunking

**Expected Overall Improvement:**
- 50-70% faster queries (with caching + partitioning)
- 30-50% less memory usage
- Better scalability

**Total Estimated Effort:** 15-25 days
