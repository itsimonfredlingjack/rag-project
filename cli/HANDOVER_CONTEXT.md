# NERDY AI CLI - Teknisk √ñverl√§mning (Handover Document)

## 1. Filstruktur

```
/home/dev/ai-server/cli/
‚îú‚îÄ‚îÄ __init__.py          # Package initialization
‚îú‚îÄ‚îÄ main.py              # Main entry point and REPL loop
‚îú‚îÄ‚îÄ client.py            # WebSocket client and backend communication
‚îú‚îÄ‚îÄ ui.py                # Rich UI components (banners, panels, streaming)
‚îú‚îÄ‚îÄ commands.py          # Slash command handlers (/juridik, /diarie, /clear, /quit)
‚îú‚îÄ‚îÄ config.py            # Configuration (backend URL, reconnect logic)
‚îî‚îÄ‚îÄ requirements.txt     # Dependencies: rich>=13.0.0, websockets>=12.0
```

## 2. K√§llkod - Frontend (main.py)

```python
"""
NERDY AI CLI - Main entry point
Cyberpunk Legal Console - Interactive REPL
"""
import asyncio
import sys
import os
from pathlib import Path
from rich.prompt import Prompt
from rich.console import Console

# Support both running as script and as module
if __name__ == "__main__" and __package__ is None:
    # Running as script: add parent directory to path
    cli_dir = Path(__file__).parent
    ai_server_dir = cli_dir.parent
    sys.path.insert(0, str(ai_server_dir))
    from cli.client import NERDYAIClient
    from cli.ui import (
        render_welcome_banner,
        show_thinking_spinner,
        show_status,
        print_separator,
        render_metadata_footer,
        get_user_prompt,
        render_agent_header,
        render_status_bar,
        create_streaming_display,
        update_streaming_display,
    )
    from cli.commands import handle_slash_command
else:
    # Running as module
    from .client import NERDYAIClient
    from .ui import (
        render_welcome_banner,
        show_thinking_spinner,
        show_status,
        print_separator,
        render_metadata_footer,
        get_user_prompt,
        render_agent_header,
        render_status_bar,
        create_streaming_display,
        update_streaming_display,
    )
    from .commands import handle_slash_command

console = Console()


async def get_user_input() -> str:
    """
    Simple user input with clean prompt
    """
    try:
        # Print simple prompt
        prompt_text = get_user_prompt()
        console.print(prompt_text, end="")
        
        # Force flush to ensure prompt is visible
        import sys
        sys.stdout.flush()
        
        # Get input
        user_input = input()
        
        # Handle empty input
        if not user_input.strip():
            return ""
        
        return user_input.strip()
    except (EOFError, KeyboardInterrupt):
        return "/quit"


async def main():
    """
    Main REPL loop with cyberpunk aesthetics
    """
    # Show welcome dashboard
    render_welcome_banner()
    
    # Show status bar
    render_status_bar()
    
    # Create client
    client = NERDYAIClient()
    
    try:
        # Connect to backend
        try:
            await client.connect()
        except ConnectionError as e:
            show_status(f"Connection failed: {e}", "error")
            show_status("Verify backend is running on ws://localhost:8000/api/chat", "info")
            sys.exit(1)
        
        # REPL loop
        while True:
            try:
                # Get user input
                user_input = await get_user_input()
                
                # Skip empty input
                if not user_input:
                    continue
                
                # Handle slash commands
                if user_input.startswith("/"):
                    should_exit = await handle_slash_command(client, user_input)
                    if should_exit:
                        break
                    continue
                
                # Send message to backend
                try:
                    await client.send_message(user_input, profile="nerdy")
                except ConnectionError as e:
                    show_status(f"Connection error: {e}", "error")
                    show_status("Attempting reconnect...", "warning")
                    try:
                        await client.connect()
                        # Retry sending message after reconnect
                        await client.send_message(user_input, profile="nerdy")
                    except Exception as reconnect_error:
                        show_status(f"Reconnect failed: {reconnect_error}", "error")
                        break
                
                # Render agent header with styling
                render_agent_header()
                
                # Create Live display for streaming
                live, accumulated_text = create_streaming_display()
                final_stats = None
                
                try:
                    # Start Live context for smooth streaming
                    with live:
                        # Iterate over chunks and accumulate text
                        async for token, stats in client.receive_stream():
                            if stats:
                                # Final stats received
                                final_stats = stats
                                break
                            if token:
                                # Update Live display with new token
                                # This accumulates text and updates Markdown
                                update_streaming_display(live, accumulated_text, token)
                    
                    # Live context ends here, final state is displayed
                    console.print()  # Blank line after response
                    
                    # Show metadata footer as Rule
                    if final_stats:
                        render_metadata_footer(final_stats)
                    else:
                        # Show footer even without stats (with mock data)
                        render_metadata_footer()
                    
                except ConnectionError as e:
                    show_status(f"Connection lost during streaming: {e}", "error")
                    show_status("Attempting reconnect...", "warning")
                    try:
                        await client.connect()
                    except Exception as reconnect_error:
                        show_status(f"Reconnect failed: {reconnect_error}", "error")
                        break
                
            except KeyboardInterrupt:
                # Ctrl+C - graceful exit
                console.print("\n")
                await handle_slash_command(client, "/quit")
                break
            except Exception as e:
                show_status(f"Unexpected error: {e}", "error")
                # Continue loop despite errors
                continue
                
    except KeyboardInterrupt:
        console.print("\n")
        await handle_slash_command(client, "/quit")
    except Exception as e:
        show_status(f"Critical error: {e}", "error")
        sys.exit(1)
    finally:
        # Close connection gracefully
        await client.close()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\n[bold bright_cyan]Session terminated. Thank you for using NERDY AI![/]")
        sys.exit(0)
```

## 3. K√§llkod - Client Logic (client.py)

```python
"""
WebSocket client for NERDY AI backend
Handles connection, streaming, and reconnect logic
"""
import asyncio
import json
from typing import AsyncGenerator, Optional
import websockets
from websockets.exceptions import (
    WebSocketException,
    ConnectionClosed,
    ConnectionClosedError,
    ConnectionClosedOK,
)

from .config import (
    get_backend_url,
    RECONNECT_DELAY,
    MAX_RECONNECT_ATTEMPTS,
    TIMEOUT,
    should_reconnect,
)
from .ui import show_status


class NERDYAIClient:
    """
    WebSocket client f√∂r NERDY AI backend.
    Hanterar:
    - Anslutning till backend
    - Reconnect-logik vid disconnect
    - Token streaming
    - Error handling
    """
    
    def __init__(self):
        self.websocket: Optional[websockets.WebSocketClientProtocol] = None
        self.url: str = get_backend_url()
        self.connected: bool = False
        self.current_mode: Optional[str] = None  # "juridik" or "diarie"
    
    async def connect(self) -> None:
        """Ansluter till backend med retry-logik"""
        attempt = 0
        
        while attempt < MAX_RECONNECT_ATTEMPTS:
            try:
                show_status(f"Ansluter till backend... (f√∂rs√∂k {attempt + 1}/{MAX_RECONNECT_ATTEMPTS})", "info")
                
                # Use asyncio.wait_for for timeout compatibility across websockets versions
                self.websocket = await asyncio.wait_for(
                    websockets.connect(self.url),
                    timeout=TIMEOUT
                )
                self.connected = True
                show_status("Ansluten till NERDY AI backend", "success")
                return
                
            except asyncio.TimeoutError:
                attempt += 1
                if attempt >= MAX_RECONNECT_ATTEMPTS:
                    show_status(f"Timeout: Kunde inte ansluta efter {TIMEOUT} sekunder", "error")
                    raise ConnectionError(f"Timeout efter {TIMEOUT} sekunder")
                show_status(f"Timeout - f√∂rs√∂ker igen om {RECONNECT_DELAY} sekunder...", "warning")
                await asyncio.sleep(RECONNECT_DELAY)
            except Exception as e:
                attempt += 1
                if attempt >= MAX_RECONNECT_ATTEMPTS:
                    show_status(f"Kunde inte ansluta efter {MAX_RECONNECT_ATTEMPTS} f√∂rs√∂k: {e}", "error")
                    raise
                
                if should_reconnect(e):
                    show_status(f"F√∂rs√∂ker √•teransluta om {RECONNECT_DELAY} sekunder...", "warning")
                    await asyncio.sleep(RECONNECT_DELAY)
                else:
                    show_status(f"Anslutningsfel: {e}", "error")
                    raise
    
    async def send_message(self, text: str, profile: str = "nerdy") -> None:
        """Skickar meddelande till backend"""
        if not self.connected or not self.websocket:
            raise ConnectionError("Inte ansluten till backend")
        
        # Build message according to Antigravity protocol
        message = {
            "text": text,
            "profile": profile
        }
        
        try:
            await self.websocket.send(json.dumps(message))
        except (ConnectionClosed, ConnectionClosedError, ConnectionClosedOK) as e:
            self.connected = False
            raise ConnectionError(f"Anslutning st√§ngd: {e}")
        except Exception as e:
            raise ConnectionError(f"Kunde inte skicka meddelande: {e}")
    
    async def receive_stream(self) -> AsyncGenerator[tuple[str, Optional[dict]], None]:
        """
        Generator som streamar tokens fr√•n backend.
        Yields: (token, stats) tuples where stats is None until final message
        """
        if not self.connected or not self.websocket:
            raise ConnectionError("Inte ansluten till backend")
        
        final_stats = None
        try:
            while True:
                try:
                    raw_message = await self.websocket.recv()
                    data = json.loads(raw_message)
                    
                    # Check if message is finished
                    if data.get("is_finished", False):
                        # Extract final stats
                        stats = data.get("stats", {})
                        if stats:
                            final_stats = stats
                        break
                    
                    # Extract token text
                    token = data.get("text", "")
                    if token:
                        yield (token, None)
                        
                except (ConnectionClosed, ConnectionClosedError, ConnectionClosedOK) as e:
                    self.connected = False
                    raise ConnectionError(f"Anslutning st√§ngd: {e}")
                except json.JSONDecodeError:
                    # Skip invalid JSON
                    continue
            
            # Yield final stats if available
            if final_stats:
                yield ("", final_stats)
                    
        except websockets.exceptions.WebSocketException as e:
            self.connected = False
            raise ConnectionError(f"WebSocket-fel: {e}")
    
    async def close(self) -> None:
        """St√§nger anslutning snyggt"""
        if self.websocket:
            try:
                await self.websocket.close()
            except Exception:
                pass
            finally:
                self.websocket = None
                self.connected = False
    
    def set_mode(self, mode: Optional[str]) -> None:
        """S√§tter aktuellt l√§ge (juridik/diarie)"""
        self.current_mode = mode
    
    def get_mode(self) -> Optional[str]:
        """H√§mtar aktuellt l√§ge"""
        return self.current_mode
```

## 4. Backend Interface

### WebSocket Endpoint
- **URL:** `ws://localhost:8000/api/chat`
- **Protocol:** Antigravity format (legacy simple format)

### Incoming Message Format (Client ‚Üí Backend)
```json
{
  "text": "user message",
  "profile": "nerdy"
}
```

### Outgoing Message Format (Backend ‚Üí Client)

**Streaming tokens (during response):**
```json
{
  "sender": "agent",
  "text": "token...",
  "is_finished": false,
  "agent_id": "nerdy",
  "model": "qwen2.5-coder:14b",
  "provider": "ollama"
}
```

**Final message (completion):**
```json
{
  "sender": "agent",
  "text": "",
  "is_finished": true,
  "model": "qwen2.5-coder:14b",
  "provider": "ollama",
  "agent_id": "nerdy",
  "stats": {
    "tokens": 714,
    "speed": 39.2,
    "duration_ms": 18200,
    "provider": "ollama",
    "model": "qwen2.5-coder:14b",
    "agent_id": "nerdy"
  }
}
```

**Notes:**
- Tokens are sent as **text chunks** (strings), not JSON objects
- Each token is a small piece of the response (could be a word, part of a word, or punctuation)
- `is_finished: false` for all streaming tokens
- `is_finished: true` only in the final message
- Stats are only included in the final message
- Backend uses `stats.tokens` and `stats.speed` (tokens per second)

## 5. Design Brief & Buggar

### M√•l: Cyberpunk/Sci-Fi Terminal Look
- **Dashboard:** Sammanh√•llen tv√•-kolumns layout i en enda Panel (fungerar bra nu)
- **Chat:** Snyggt formatterad med f√§rger, tydlig separation mellan anv√§ndare och AI
- **Streaming:** Live text som fl√∂dar in smidigt med syntax highlighting
- **Metadata:** Diskret Rule med stats efter varje svar

### K√§nda Problem (Kritiska Buggar)

#### 1. "Scenskr√§ck" - Streaming Buggen
**Symptom:**
- Texten f√∂rsvinner eller klipps av ("testo", "g", "te...")
- Ibland syns bara f√∂rsta bokstaven, sen inget mer
- AI:ns svar renderas inte korrekt

**Nuvarande Implementation:**
- Anv√§nder `rich.live.Live` med `Markdown`-objekt
- `create_streaming_display()` skapar Live context
- `update_streaming_display()` ackumulerar text och uppdaterar Markdown
- Problem: Markdown-objektet uppdateras men texten syns inte eller f√∂rsvinner

**Trolig orsak:**
- Live context uppdateras f√∂r snabbt eller krockar med console output
- Markdown-objektet renderas inte korrekt i Live context
- Panel med MINIMAL box kan d√∂lja texten

#### 2. Design - F√∂r platt/tr√•kig chat
**Symptom:**
- Chatten ser ut som vanlig text (vit p√• svart)
- Ingen visuell struktur eller f√§rgkodning i sj√§lva chat-meddelandena
- Anv√§ndarens input och AI:ns svar ser likadana ut

**Nuvarande Implementation:**
- User prompt: `[bold cyan]üë§ USER[/] [dim]@[/] [cyan]CASE-FILE[/] [bold bright_white]‚ùØ[/]`
- AI header: `[bold magenta]ü§ñ NERDY AI[/] [dim]processing...[/]`
- AI response: I Panel med MINIMAL box, bright_white text
- Problem: Texten i Panel syns inte eller √§r f√∂r diskret

#### 3. Layout - Separata boxar
**Status:** ‚úÖ FIXAT - Dashboarden anv√§nder nu Table.grid inuti en enda Panel

### √ñnskad L√∂sning

1. **Streaming:**
   - Text ska synas direkt n√§r den kommer fr√•n backend
   - Hela svaret ska renderas, inte bara f√∂rsta bokstaven
   - Anv√§nd Rich-komponenter f√∂r snygg formatering (Markdown, syntax highlighting)
   - Inga blinkningar eller f√∂rsvinnande text

2. **Design:**
   - Tydlig visuell separation: Anv√§ndare (cyan) vs AI (magenta)
   - F√§rgkodad text i AI-svaren (inte bara vit)
   - Struktur utan tunga boxar (anv√§nd Padding, MINIMAL borders)
   - Cyberpunk-k√§nsla med neon-f√§rger

3. **Metadata:**
   - Diskret Rule med stats (fungerar bra nu)
   - Tydlig avdelare mellan interaktioner

### Tekniska Krav

- **Bibliotek:** `rich` (>=13.0.0) - anv√§nd Rich-komponenter, inte vanlig print()
- **Streaming:** M√•ste fungera med `rich.live.Live` eller alternativ metod
- **F√§rger:** Anv√§nd Rich's f√§rgsystem (bright_cyan, bright_magenta, etc.)
- **Struktur:** Anv√§nd Panel, Padding, Markdown f√∂r formatering
- **Performance:** Streaming m√•ste vara smidig, ingen lagg eller blinkning

### Test-scenario

1. Starta CLI: `python3 -m cli.main`
2. Skriv: `test`
3. F√∂rv√§ntat resultat:
   - AI:ns svar ska streamas in tecken f√∂r tecken
   - Hela svaret ska synas (inte bara "t" eller "te")
   - Text ska vara f√§rgkodad och snyggt formaterad
   - Metadata ska visas som Rule efter svaret

---

**Sammanfattning f√∂r Gemini:**
"Fix streaming-buggen d√§r texten f√∂rsvinner. G√∂r chatten snyggt formatterad med Rich-komponenter. Beh√•ll dashboarden i toppen (den fungerar bra). Anv√§nd f√§rger, struktur och tydlig separation mellan anv√§ndare och AI. M√•let √§r en Cyberpunk-terminal som k√§nns proffsig och fungerar."
