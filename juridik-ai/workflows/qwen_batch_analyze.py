#!/usr/bin/env python3
"""
Batch-analysera juridiska dokument med Qwen 2.5 3B via Ollama.
K√∂r sekventiellt f√∂r att respektera GPU-begr√§nsningar p√• RTX 2060.
"""

import json
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

# F√∂rs√∂k importera ollama, annars anv√§nd curl
try:
    import ollama

    USE_OLLAMA_LIB = True
except ImportError:
    USE_OLLAMA_LIB = False
    print("‚ö†Ô∏è ollama-python ej installerat, anv√§nder curl")


# Konfiguration
MODEL = "qwen-myndighet"  # Custom model f√∂r myndighetssverige
FALLBACK_MODEL = "qwen2.5:3b-instruct"
SYSTEM_PROMPT_PATH = Path("/home/dev/juridik-ai/system-prompts/qwen-myndighet.txt")

# RTX 2060 optimerade inst√§llningar
OPTIONS = {
    "num_ctx": 4096,
    "num_predict": 2048,
    "temperature": 0.3,
    "top_p": 0.9,
    "repeat_penalty": 1.1,
}


def load_system_prompt() -> str:
    """Ladda system prompt fr√•n fil."""
    if SYSTEM_PROMPT_PATH.exists():
        return SYSTEM_PROMPT_PATH.read_text(encoding="utf-8")
    return "Du √§r en svensk juridisk assistent."


def analyze_with_ollama(content: str, task: str = "loggbok") -> dict:
    """Analysera dokument med ollama-python biblioteket."""
    system_prompt = load_system_prompt()

    task_prompts = {
        "loggbok": "Analysera dokumentet och skapa en myndighetsloggbok enligt standardformatet.",
        "risk": "Identifiera risker och brister i myndighetens hantering. Lista varje risk med niv√• (L√•g/Medel/H√∂g).",
        "sammanfatta": "Sammanfatta dokumentets inneh√•ll ur ett medborgarperspektiv p√• max 200 ord.",
        "brister": "Lista dokumentationsbrister, handl√§ggningsfel och saknade handlingar.",
    }

    prompt = f"""
{task_prompts.get(task, task_prompts['loggbok'])}

DOKUMENT:
{content}

OUTPUT:
"""

    try:
        # F√∂rs√∂k med custom model f√∂rst
        response = ollama.generate(model=MODEL, prompt=prompt, options=OPTIONS)
        return {
            "success": True,
            "model": MODEL,
            "response": response["response"],
            "tokens": response.get("eval_count", 0),
        }
    except Exception as e:
        if "not found" in str(e).lower():
            # Fallback till bas-modell
            response = ollama.chat(
                model=FALLBACK_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt},
                ],
                options=OPTIONS,
            )
            return {
                "success": True,
                "model": FALLBACK_MODEL,
                "response": response["message"]["content"],
                "tokens": response.get("eval_count", 0),
            }
        raise


def analyze_with_curl(content: str, task: str = "loggbok") -> dict:
    """Analysera dokument med curl (fallback om ollama-python saknas)."""
    system_prompt = load_system_prompt()

    task_prompts = {
        "loggbok": "Analysera dokumentet och skapa en myndighetsloggbok enligt standardformatet.",
        "risk": "Identifiera risker och brister i myndighetens hantering.",
        "sammanfatta": "Sammanfatta dokumentets inneh√•ll ur ett medborgarperspektiv p√• max 200 ord.",
        "brister": "Lista dokumentationsbrister, handl√§ggningsfel och saknade handlingar.",
    }

    prompt = f"{task_prompts.get(task, task_prompts['loggbok'])}\n\nDOKUMENT:\n{content}\n\nOUTPUT:"

    payload = {
        "model": FALLBACK_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt},
        ],
        "stream": False,
        "options": OPTIONS,
    }

    result = subprocess.run(
        ["curl", "-s", "-X", "POST", "http://localhost:11434/api/chat", "-d", json.dumps(payload)],
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        return {"success": False, "error": result.stderr}

    response = json.loads(result.stdout)
    return {
        "success": True,
        "model": FALLBACK_MODEL,
        "response": response.get("message", {}).get("content", ""),
        "tokens": response.get("eval_count", 0),
    }


def analyze_document(content: str, task: str = "loggbok") -> dict:
    """Analysera dokument med tillg√§nglig metod."""
    if USE_OLLAMA_LIB:
        return analyze_with_ollama(content, task)
    else:
        return analyze_with_curl(content, task)


def batch_analyze(input_dir: Path, output_dir: Path, task: str = "loggbok"):
    """Batch-analysera alla dokument i en katalog."""
    output_dir.mkdir(parents=True, exist_ok=True)

    # Hitta alla textfiler
    files = list(input_dir.glob("*.txt")) + list(input_dir.glob("*.md"))

    if not files:
        print(f"Inga dokument hittades i {input_dir}")
        return

    print(f"üîç Hittade {len(files)} dokument att analysera")
    print(f"üìù Analystyp: {task}")
    print(f"ü§ñ Modell: {MODEL} (fallback: {FALLBACK_MODEL})")
    print("-" * 50)

    results = []
    start_time = time.time()

    for i, file in enumerate(files, 1):
        print(f"\n[{i}/{len(files)}] Analyserar: {file.name}")

        # L√§s dokument
        content = file.read_text(encoding="utf-8")
        print(f"  üìÑ Storlek: {len(content)} tecken (~{len(content)//4} tokens)")

        # Analysera
        doc_start = time.time()
        result = analyze_document(content, task)
        doc_time = time.time() - doc_start

        if result["success"]:
            # Spara resultat
            output_file = output_dir / f"{file.stem}_analys.md"

            output_content = f"""# Juridisk Analys: {file.name}

**Genererad:** {datetime.now().strftime('%Y-%m-%d %H:%M')}
**Modell:** {result['model']}
**Analystyp:** {task}
**Tokens:** ~{result['tokens']}

---

{result['response']}

---
*Analyserad med juridik-ai pipeline*
"""
            output_file.write_text(output_content, encoding="utf-8")

            print(f"  ‚úÖ Klar ({doc_time:.1f}s) ‚Üí {output_file.name}")
            results.append(
                {"file": file.name, "success": True, "time": doc_time, "output": output_file.name}
            )
        else:
            print(f"  ‚ùå Fel: {result.get('error', 'Ok√§nt fel')}")
            results.append({"file": file.name, "success": False, "error": result.get("error")})

    # Sammanfattning
    total_time = time.time() - start_time
    successful = sum(1 for r in results if r["success"])

    print("\n" + "=" * 50)
    print("BATCH-ANALYS KLAR")
    print("=" * 50)
    print(f"‚úÖ Lyckade: {successful}/{len(files)}")
    print(f"‚è±Ô∏è Total tid: {total_time:.1f}s")
    print(f"üìÅ Output: {output_dir}/")

    # Spara sammanfattning
    summary_file = output_dir / "_batch_summary.json"
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(
            {
                "timestamp": datetime.now().isoformat(),
                "task": task,
                "total_files": len(files),
                "successful": successful,
                "total_time_seconds": total_time,
                "results": results,
            },
            f,
            indent=2,
            ensure_ascii=False,
        )

    return results


def main():
    """CLI f√∂r batch-analys."""
    if len(sys.argv) < 2:
        print("Anv√§ndning: python qwen_batch_analyze.py <input_dir> [output_dir] [task]")
        print("\nTasks: loggbok, risk, sammanfatta, brister")
        print("\nExempel:")
        print("  python qwen_batch_analyze.py ./sections ./output loggbok")
        sys.exit(1)

    input_dir = Path(sys.argv[1])
    output_dir = Path(sys.argv[2]) if len(sys.argv) > 2 else Path("./output")
    task = sys.argv[3] if len(sys.argv) > 3 else "loggbok"

    if not input_dir.exists():
        print(f"Fel: Katalogen {input_dir} finns inte")
        sys.exit(1)

    batch_analyze(input_dir, output_dir, task)


if __name__ == "__main__":
    main()
