# Optimering av lokala AI-modeller

## Typer av optimering:

### 1. **Quantization (Kvantisering)** - Minska modellstorlek
- **4-bit quantization**: Minskar storlek med ~75%, lite s√§mre kvalitet
- **8-bit quantization**: Minskar storlek med ~50%, b√§ttre kvalitet
- **Exempel**: Gemma3:4b (3.3GB) ‚Üí Gemma3:4b-Q4 (0.8GB)

**F√∂rdelar:**
- Mindre VRAM-anv√§ndning
- Snabbare laddning
- Mer plats f√∂r st√∂rre context

**Nackdelar:**
- Lite s√§mre kvalitet
- Kan vara l√•ngsammare p√• vissa GPU:er

### 2. **Context Length** - Justera minne
- **Nuvarande**: 4096 tokens (~3000 ord)
- **√ñka**: 8192, 16384 tokens (mer minne, mer VRAM)
- **Minska**: 2048 tokens (mindre VRAM, snabbare)

**F√∂rdelar med st√∂rre context:**
- Kan h√•lla mer i minnet
- B√§ttre f√∂r l√•nga konversationer

**Nackdelar:**
- Mer VRAM-anv√§ndning
- L√•ngsammare

### 3. **Temperature & Top_P** (Vi har redan optimerat!)
- **Temperature**: 0.3 (l√§gre = mer fokuserat, h√∂gre = mer kreativt)
- **Top_P**: 0.8 (l√§gre = mer f√∂ruts√§gbart, h√∂gre = mer varierat)

### 4. **Ollama-specifika inst√§llningar**
- **num_ctx**: Context length (4096, 8192, etc.)
- **num_gpu**: Antal GPU-lager att anv√§nda
- **num_thread**: CPU-tr√•dar (om delvis CPU)

### 5. **GPU-optimering**
- **CUDA settings**: Justera GPU-minneshantering
- **Batch size**: F√∂r batch-inferens (inte relevant f√∂r chat)

## Praktiska optimeringar f√∂r din setup:

### A. **Quantized modeller** (om du vill ha mindre/snabbare):

```bash
# Kolla om det finns quantized versioner
ollama pull gemma3:4b-q4_0  # 4-bit (mycket mindre)
ollama pull gemma3:4b-q8_0  # 8-bit (lite mindre)
```

### B. **Justera context length** (i config.toml):

```toml
[model]
context_length = 8192  # √ñka f√∂r mer minne
# eller
context_length = 2048  # Minska f√∂r snabbare
```

### C. **Ollama environment variables**:

```bash
# √ñka GPU-anv√§ndning
export OLLAMA_NUM_GPU=1
export OLLAMA_NUM_CTX=4096

# Justera CPU-tr√•dar (om delvis CPU)
export OLLAMA_NUM_THREAD=4
```

### D. **System prompt optimering** (Vi har redan gjort!)
- Kortfattat = snabbare svar
- Tydliga instruktioner = b√§ttre resultat

## Rekommendationer f√∂r din RTX 2060 (6GB VRAM):

### ‚úÖ **Redan optimerat:**
- Temperature: 0.3 (fokuserat)
- Top_P: 0.8 (balanserat)
- System prompt: Kortfattat, tydligt

### üí° **Ytterligare optimeringar du kan testa:**

1. **Testa quantized version** (om den finns):
   ```bash
   ollama pull gemma3:4b-q4_0
   # Uppdatera config.toml: name = "gemma3:4b-q4_0"
   ```
   - Mycket mindre VRAM
   - Snabbare
   - Lite s√§mre kvalitet

2. **Justera context** (om du beh√∂ver mer minne):
   ```toml
   context_length = 8192  # Mer minne
   ```

3. **GPU-optimering** (i ~/.bashrc eller ~/.zshrc):
   ```bash
   export OLLAMA_NUM_GPU=1
   export OLLAMA_GPU_LAYERS=35  # Antal lager p√• GPU
   ```

## Testa prestanda:

```bash
# Testa nuvarande
time ollama run gemma3:4b "Test"

# Testa quantized (om tillg√§nglig)
time ollama run gemma3:4b-q4_0 "Test"

# J√§mf√∂r hastighet och kvalitet
```

## Viktigt:

- **Quantization**: Bra f√∂r att spara VRAM, men kan p√•verka kvalitet
- **Context**: St√∂rre = mer minne men mer VRAM
- **Temperature**: Vi har redan optimerat (0.3)
- **System prompt**: Vi har redan optimerat

## F√∂r din setup:

**Nuvarande optimering √§r bra!** Gemma3:4b passar perfekt f√∂r din GPU.

**Om du vill testa mer:**
- Kolla om det finns `gemma3:4b-q4_0` eller `gemma3:4b-q8_0`
- Testa och j√§mf√∂r hastighet/kvalitet
